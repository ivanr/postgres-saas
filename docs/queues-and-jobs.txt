
Queues and Jobs
===============

  - job_id
  - scheduled_job_id -- if reusing the same row for scheduling and actual jobs
  - created_on
  - last_success
  - last_attempt -- includes failures
  - next_scheduled -- this is when the job is supposed to be run
  - next_attempt -- this is the next attempt; if the job fails, next_scheduled stays the same
  - failed_attempts (since the last success)
  - priority -- integer from 0 to 10, default 5; maybe we want to support more elaborate strategies? For example have a fairness_key?
  - status -- healthy, failed
  - status_message
  - status_object JSONB -- for arbitrary status information
  - duration statistics: last duration, averages in the last M, N, P periods, total runs, total time
  - deadline
  - fingerprint (for deduplication)

  + Scheduling metadata, for example: fixed rate, fixed delay, interval, cron schedule,
       how to reschedule after failures, etc

  + How do we know when to schedule a job in relation to other jobs? A job
    may have some resource requirements and compete with other jobs?

  + We also need to know what the job is. This will sometimes be understood
    from the context, but if we're building a generic job scheduling feature,
    we need some more information. For example:

    - type
    - input -- probably as JSON
    - input_payload -- for additional data, especially binary; JSON is inefficient for that
    - input_payload_ct -- (ct => content type)

  + We probably want to support multiple queues, but those should probably be implemented
    as separate tables using Postgres inheritance.

  + Use partitioning to minimise bloat; can partman should be able to be configured
    to drop partitions only if they're empty?

  + If not using partitioning, bloat can be fixed by copying the data into a temporary
    table, running truncate, then copying the data back.

  + Do we want to keep a record of job attempts and invocations? How about job results?
    Probably in a separate table where we can have custom retention rules.

- Are queues jobs that are immediately due?

- TODO Job deduplication


Thoughts
--------

A Postgres queue needs to deal with three problems:

- Pulling jobs without conflict

  1. Use one puller per queue; in this case conflict is avoided with the design. Pullers
     can carry out the work themselves or delegate it to workers.

     - It's also possible to shard the jobs, then use multiple pullers, one per
       shard. Each puller can obtain a lease on a shard. If the lease is not
       renewed, another puller can take over.

  2. Pullers take one job at a time, using SELECT ... FOR UPDATE, SKIP LOCKED. Requires
     one database connection per puller, which may or may not be prohibitive. This
     approach is natively HA if the workers are spread across many computers.

  3. Rows are retrieved for UPDATE, SKIP LOCKED, then updated with in-progress status
     and a timestamp. Rows that timeout can be selected by other pullers. Long jobs
     will need to refresh the timestamp. Jobs in progress are updated with a random
     token, only workers who have the token can update. Or the lease expires.

     Bonus with this approach: we know which jobs are in progress and how long. The
     table could be extended to carry status information.

- Failures

  It's likely that some jobs will fail, what do we do then?

    - We can ignore this problem and let the applications deal with it; for example,
      they could move the jobs into a dead letter table.

    - If we want to retry later, we can keep a try-next timestamp and index based on that.

    - If we partition based on that timestamp, Postgres will move jobs across partitions
      as necessary.

    - We could have a separate dead-letter queue for permanent failures. So perhaps
      partitioning at two levels: first based on status (active and dead), then based
      on try-next timestamp (active only).

- Bloat

  Bloat happens on tables with frequent updates. We want to avoid it. If we use partitions,
  we can avoid deletions and can drop entire partitions at a time. Even if we delete, when
  we drop partitions we avoid bloat as well.

  Partitions also give us efficient archival if we don't delete jobs, we can keep the
  record of their running for a period of time.

- References

  - https://www.foundationdb.org/files/QuiCK.pdf

  - https://temporal.io

  - https://github.com/tembo-io/pgmq

  - https://hatchet.run

  - https://aws.amazon.com/builders-library/avoiding-insurmountable-queue-backlogs/

  - https://muatik.medium.com/my-notes-on-avoiding-insurmountable-queue-backlogs-a39fa39306fd

  - https://www.postgresql.org/docs/current/storage-hot.html

  - https://dataegret.com/2023/07/automated-index-bloat-management-how-pg_index_watch-keeps-postgresql-indexes-lean/

  - https://www.timescale.com/learn/how-to-reduce-bloat-in-large-postgresql-tables

  - https://docs.hatchet.run/blog/multi-tenant-queues

  - https://www.cs.bu.edu/fac/matta/Teaching/cs655-papers/DRR.pdf